diff --git a/src/pipecat/processors/frame_processor.py b/src/pipecat/processors/frame_processor.py
index 9f614ba..e7d3e0a 100644
--- a/src/pipecat/processors/frame_processor.py
+++ b/src/pipecat/processors/frame_processor.py
@@ -245,7 +245,8 @@ class FrameProcessor:
                 logger.trace(f"Pushing {frame} upstream from {self} to {self._prev}")
                 await self._prev.queue_frame(frame, direction)
         except Exception as e:
-            logger.exception(f"Uncaught exception in {self}: {e}")
+            logger.exception(f"<>      Uncaught exception in {self}: {e}")
+            raise Exception("internal_push_frame") from e
 
     def __create_input_task(self):
         self.__input_queue = asyncio.Queue()
diff --git a/src/pipecat/services/ai_services.py b/src/pipecat/services/ai_services.py
index e0f16e2..5c374a0 100644
--- a/src/pipecat/services/ai_services.py
+++ b/src/pipecat/services/ai_services.py
@@ -289,6 +289,7 @@ class TTSService(AIService):
 
     async def process_frame(self, frame: Frame, direction: FrameDirection):
         await super().process_frame(frame, direction)
+        print(f"_---_ai_services.py process_frame * frame: {frame}")
 
         if isinstance(frame, TextFrame):
             await self._process_text_frame(frame)
@@ -343,6 +344,7 @@ class TTSService(AIService):
             await self._push_tts_frames(text)
 
     async def _push_tts_frames(self, text: str):
+        print(f"_____ai_services.py * push_tts_frames str: {str}")
         # Don't send only whitespace. This causes problems for some TTS models. But also don't
         # strip all whitespace, as whitespace can influence prosody.
         if not text.strip():
diff --git a/src/pipecat/services/cartesia.py b/src/pipecat/services/cartesia.py
index 7fad02f..276b172 100644
--- a/src/pipecat/services/cartesia.py
+++ b/src/pipecat/services/cartesia.py
@@ -137,6 +137,7 @@ class CartesiaTTSService(WordTTSService):
     def _build_msg(
         self, text: str = "", continue_transcript: bool = True, add_timestamps: bool = True
     ):
+        print(f"_____cartesia.py * _build_msg str: {str}")
         voice_config = {}
         voice_config["mode"] = "id"
         voice_config["id"] = self._voice_id
diff --git a/src/pipecat/services/openai.py b/src/pipecat/services/openai.py
index b6927e8..df9715b 100644
--- a/src/pipecat/services/openai.py
+++ b/src/pipecat/services/openai.py
@@ -166,6 +166,7 @@ class BaseOpenAILLMService(LLMService):
         params.update(self._settings["extra"])
 
         chunks = await self._client.chat.completions.create(**params)
+        print(f"_____openai.py get_chat_completions * chunks: {chunks}")
         return chunks
 
     async def _stream_chat_completions(
@@ -202,6 +203,7 @@ class BaseOpenAILLMService(LLMService):
         function_name = ""
         arguments = ""
         tool_call_id = ""
+        print(f"_____openai.py * _process_context: tool_call_id", tool_call_id)
 
         await self.start_ttfb_metrics()
 
@@ -210,6 +212,7 @@ class BaseOpenAILLMService(LLMService):
         )
 
         async for chunk in chunk_stream:
+            # print(f"___*__openai.py * chunk: {chunk}")
             if chunk.usage:
                 tokens = LLMTokenUsage(
                     prompt_tokens=chunk.usage.prompt_tokens,
@@ -237,7 +240,7 @@ class BaseOpenAILLMService(LLMService):
                 # We accumulate all the arguments for the rest of the streamed response, then when
                 # the response is done, we package up all the arguments and the function name and
                 # yield a frame containing the function name and the arguments.
-
+                print(f"___________________________openai.py * tool_call_id: {tool_call_id}")
                 tool_call = chunk.choices[0].delta.tool_calls[0]
                 if tool_call.index != func_idx:
                     functions_list.append(function_name)
@@ -503,6 +506,7 @@ class OpenAIAssistantContextAggregator(LLMAssistantContextAggregator):
 
     async def process_frame(self, frame, direction):
         await super().process_frame(frame, direction)
+        print(f"___<1>__openai.py * OpenAIAssistantContextAggregator process_frame frame : {frame}")
         # See note above about not calling push_frame() here.
         if isinstance(frame, StartInterruptionFrame):
             self._function_calls_in_progress.clear()
@@ -525,11 +529,14 @@ class OpenAIAssistantContextAggregator(LLMAssistantContextAggregator):
         elif isinstance(frame, OpenAIImageMessageFrame):
             self._pending_image_frame_message = frame
             await self._push_aggregation()
+        else:
+            print(f"___<2>__openai.py * OpenAIAssistantContextAggregator process_frame frame : {frame}")
 
     async def _push_aggregation(self):
         if not (
             self._aggregation or self._function_call_result or self._pending_image_frame_message
         ):
+            print(f"_____openai.py * return from push_aggregation__________")
             return
 
         run_llm = False
